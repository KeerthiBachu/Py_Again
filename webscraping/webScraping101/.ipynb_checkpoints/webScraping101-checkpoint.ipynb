{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping 101 | Introductory Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference: [Web Scraping 101 with Python](http://www.gregreda.com/2013/03/03/web-scraping-101-with-python/) by [Greg Reda](https://twitter.com/gjreda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "*Wed Sep  7 15:34:17 IST 2016*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For this example, we're going to use the Chicago Reader's Best of 2011 list. Why? Because I think it's a great example of terrible data presentation on the web. Go ahead and browse it for a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To start, we need to take a look at the HTML that displays these categories. If you're in Chrome or Firefox, highlight \"Readers' Poll Winners\", right-click, and select Inspect Element.\n",
    "\n",
    "> This opens up the browser's Developer Tools (in Firefox, you might now have to click the HTML button on the right side of the developer pane to fully show it). Now we'll be able to see the page layout. The browser has brought us directly to the piece of HTML that's used to display the \"Readers' Poll Winners\" `<dt>` element.\n",
    "\n",
    "> This seems to be the area of code where there's going to be some consistency in how the category links are displayed. See that `<dl class=\"boccat\">` just above our \"Readers' Poll Winners\" line? If you mouse over that line in your browser's dev tools, you'll notice that it highlights the entire section of category links we want. And every category link is within a `<dd>` element. Perfect! Let's get all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib2 import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASE_URL = \"https://www.chicagoreader.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_category_links(section_url):\n",
    "    html = urlopen(section_url).read()\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    boccat = soup.find(\"dl\", \"boccat\")\n",
    "    category_links = [BASE_URL + dd.a[\"href\"] for dd in boccat.findAll(\"dd\")]\n",
    "    return category_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Hopefully this code is relatively easy to follow, but if not, here's what we're doing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loading the urlopen function from the urllib2 library into our local namespace.\n",
    "- Loading the BeautifulSoup class from the bs4 (BeautifulSoup4) library into our local namespace.\n",
    "- Setting a variable named BASE_URL to `http://www.chicagoreader.com`. We do this because the links used through the site are relative - meaning they do not include the base domain. In order to store our links properly, we need to concatenate the base domain with each relative link.\n",
    "- Define a function named get_category_links.\n",
    "  - The function requires a parameter of `section_url`. In this example, we are going to use the **Food and Drink** section of the BOC list, however we could use a different section URL: for instance, the **City Life** section's URL. We're able to create just one generic function because each section page is structured the same.\n",
    "  - Open the `section_url` and read it in the html object.\n",
    "  - Create an object called soup based on the BeautifulSoup class. The soup object is an instance of the BeautifulSoup class. It is initialized with the `html` object and parsed with `lxml`.\n",
    "  - In our BeautifulSoup instance (which we called `soup`), find the `<dl>` element with a class of \"boccat\" and store that section in a variable called boccat.\n",
    "  - This is a list comprehension. For every `<dd>` element found within our boccat variable, we're getting the `href` of its `<a>` element (our category links) and concatenating on our `BASE_URL` to make it a complete link. All of these links are being stored in a list called `category_links`. You could also write this line with a for loop, but I prefer a list comprehension here because of its simplicity.\n",
    "  - Finally, our function returns the `category_links` list that we created on the previous line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now that we have our list of category links, we'd better start going through them to get our winners and runners-up. Let's figure out which elements contain the parts we care about.\n",
    "\n",
    "> If we look at the Best Chef category, we can see that our category is in `<h1 class=\"headline\">`. Shortly after that, we find our winner and runners-up stored in `<h2 class=\"boc1\">` and `<h2 class=\"boc2\">`, respectively.\n",
    "\n",
    "> Let's write some code to get all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_category_winner(category_url):\n",
    "    html = urlopen(category_url).read()\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    category = soup.find(\"h1\", \"headline\").string\n",
    "    winner = [h2.string for h2 in soup.findAll(\"h2\", \"boc1\")]\n",
    "    runners_up = [h2.string for h2 in soup.findAll(\"h2\", \"boc2\")]\n",
    "    return {\"category\": category, \"category_url\": category_url, \"winner\": winner, \"runners_up\": runners_up}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DRY - Don't Repeat Yourself\n",
    "> All the text in this notebook includig the paras below are copied directly from the link mentioned in the Reference above.\n",
    "\n",
    "As mentioned in the previous section, lines two and three of our second function mirror lines in our first function.\n",
    "\n",
    "Imagine a scenario where we want to change the parser we're passing into our BeautifulSoup instance (in this case, lxml). With the way we've currently written our code, we'd have to make that change in two places. Now imagine you've written many more functions to scrape this data - maybe one to get addresses and another to get paragraphs of text about the winner - you've likely repeated those same two lines of code in these functions and you now have to remember to make changes in four different places. That's not ideal.\n",
    "\n",
    "**A good principle in writing code is DRY - Don't Repeat Yourself**. \n",
    "\n",
    "When you notice that you've written the same lines of code a couple times throughout your script, it's probably a good idea to step back and think if there's a better way to structure that piece."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In our case, we're going to write another function to\n",
    "simply process a URL and return a BeautifulSoup instance.\n",
    "We can then call this function in our other functions\n",
    "instead of duplicating our logic.\n",
    "\"\"\"\n",
    "def make_soup(url):\n",
    "    html = urlopen(url).read()\n",
    "    return BeautifulSoup(html, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And also make this change in the above function definitions.\n",
    "<code>\n",
    "soup = make_soup(url)\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "URLError",
     "evalue": "<urlopen error [Errno 111] Connection refused>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-288102bc2a61>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# a list to store our dictionaries\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcategory\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcategories\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mwinner\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_category_winner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwinner\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# be nice\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-288102bc2a61>\u001b[0m in \u001b[0;36mget_category_winner\u001b[1;34m(category_url)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_category_winner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategory_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_soup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategory_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mcategory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"h1\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"headline\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mwinner\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mh2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mh2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"h2\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"boc1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-288102bc2a61>\u001b[0m in \u001b[0;36mmake_soup\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmake_soup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mhtml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"lxml\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/as/anaconda2/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/as/anaconda2/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    429\u001b[0m             \u001b[0mreq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 431\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    432\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m         \u001b[1;31m# post-process response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/as/anaconda2/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36m_open\u001b[1;34m(self, req, data)\u001b[0m\n\u001b[0;32m    447\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[1;32m--> 449\u001b[1;33m                                   '_open', req)\n\u001b[0m\u001b[0;32m    450\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/as/anaconda2/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36m_call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    407\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 409\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    410\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/as/anaconda2/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36mhttps_open\u001b[1;34m(self, req)\u001b[0m\n\u001b[0;32m   1238\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mhttps_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1239\u001b[0m             return self.do_open(httplib.HTTPSConnection, req,\n\u001b[1;32m-> 1240\u001b[1;33m                 context=self._context)\n\u001b[0m\u001b[0;32m   1241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1242\u001b[0m         \u001b[0mhttps_request\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAbstractHTTPHandler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_request_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/as/anaconda2/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36mdo_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1195\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# XXX what error?\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m             \u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1197\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1198\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1199\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mURLError\u001b[0m: <urlopen error [Errno 111] Connection refused>"
     ]
    }
   ],
   "source": [
    "# Therefore ... the final code is ...\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib2 import urlopen\n",
    "BASE_URL = \"https://www.chicagoreader.com\"\n",
    "\n",
    "def make_soup(url):\n",
    "    html = urlopen(url).read()\n",
    "    return BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "def get_category_links(section_url):\n",
    "    soup = make_soup(section_url)\n",
    "    boccat = soup.find(\"dl\", \"boccat\")\n",
    "    category_links = [BASE_URL + dd.a[\"href\"] for dd in boccat.findAll(\"dd\")]\n",
    "    return category_links\n",
    "\n",
    "def get_category_winner(category_url):\n",
    "    soup = make_soup(category_url)\n",
    "    category = soup.find(\"h1\", \"headline\").string\n",
    "    winner = [h2.string for h2 in soup.findAll(\"h2\", \"boc1\")]\n",
    "    runners_up = [h2.string for h2 in soup.findAll(\"h2\", \"boc2\")]\n",
    "    return {\"category\": category, \"category_url\": category_url, \"winner\": winner, \"runners_up\": runners_up}\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    food_n_drink = (\"http://www.chicagoreader.com/chicago/\"\n",
    "                    \"best-of-chicago-2011-food-drink/BestOf?oid=4106228\")\n",
    "    \n",
    "    categories = get_category_links(food_n_drink)\n",
    "\n",
    "    data = [] # a list to store our dictionaries\n",
    "    for category in categories:\n",
    "        winner = get_category_winner(category)\n",
    "        data.append(winner)\n",
    "        sleep(1) # be nice\n",
    "\n",
    "print data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is author's code\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib2 import urlopen\n",
    "from time import sleep # be nice\n",
    "\n",
    "BASE_URL = \"http://www.chicagoreader.com\"\n",
    "\n",
    "def make_soup(url):\n",
    "    html = urlopen(url).read()\n",
    "    return BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "def get_category_links(section_url):\n",
    "    soup = make_soup(section_url)\n",
    "    boccat = soup.find(\"dl\", \"boccat\")\n",
    "    category_links = [BASE_URL + dd.a[\"href\"] for dd in boccat.findAll(\"dd\")]\n",
    "    return category_links\n",
    "\n",
    "def get_category_winner(category_url):\n",
    "    soup = make_soup(category_url)\n",
    "    category = soup.find(\"h1\", \"headline\").string\n",
    "    winner = [h2.string for h2 in soup.findAll(\"h2\", \"boc1\")]\n",
    "    runners_up = [h2.string for h2 in soup.findAll(\"h2\", \"boc2\")]\n",
    "    return {\"category\": category,\n",
    "            \"category_url\": category_url,\n",
    "            \"winner\": winner,\n",
    "            \"runners_up\": runners_up}\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    food_n_drink = (\"http://www.chicagoreader.com/chicago/\"\n",
    "                    \"best-of-chicago-2011-food-drink/BestOf?oid=4106228\")\n",
    "    \n",
    "    categories = get_category_links(food_n_drink)\n",
    "\n",
    "    data = [] # a list to store our dictionaries\n",
    "    for category in categories:\n",
    "        winner = get_category_winner(category)\n",
    "        data.append(winner)\n",
    "        sleep(1) # be nice\n",
    "\n",
    "    print data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Experiment\n",
    "\n",
    "Now, with the above code as it is, I just created a gist link.\n",
    "\n",
    "https://gist.github.com/asinode/16ee953f91df4817f6c73e461b80eb9a\n",
    "\n",
    "I will try to get the code out of this page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib2 import urlopen\n",
    "\n",
    "base_url = \"https://gist.github.com/asinode/16ee953f91df4817f6c73e461b80eb9a\"\n",
    "\n",
    "html = urlopen(base_url).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40888"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ead prefix=\"og: http://ogp.me/ns# fb: http://ogp.me/ns/fb# object: http://ogp.me/ns/object# article:'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html[50:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "htmllist = html.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(htmllist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2575"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(htmllist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<!DOCTYPE',\n",
       " 'html>',\n",
       " '<html',\n",
       " 'lang=\"en\"',\n",
       " 'class=\"\">',\n",
       " '<head',\n",
       " 'prefix=\"og:',\n",
       " 'http://ogp.me/ns#',\n",
       " 'fb:',\n",
       " 'http://ogp.me/ns/fb#']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "htmllist[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tdata = soup.find(\"div\", \"js-gist-file-update-container js-task-list-container file-box\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.Tag"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print tdata # looking at the output, this is not what I want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
